{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WCamaioni/Fatec_PLN_Codes/blob/main/Aula_8%5BPLN%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 08 - Introdução a Machine Learning para PLN\n",
        "**Objetivo da aula:**\n",
        "*  Introduzir os fundamentos de aprendizado de máquina aplicados ao PLN, focando na classificação de texto e regressão, utilizando algoritmos como Naive Bayes e SVM."
      ],
      "metadata": {
        "id": "j8udKF6jOv4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 01 - Aplicação do modelo de Naives em um texto"
      ],
      "metadata": {
        "id": "26SHwQhjO-yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1 - Criar o Corpus\n",
        "\n",
        "corpus = [\n",
        "    (\"Eu amo PLN\",\"positivo\"),\n",
        "    (\"Eu odeio bugs\",\"negativo\"),\n",
        "    (\"Amo resolver problemas\",\"positivo\"),\n",
        "    (\"Odeio erros\",\"negativo\"),\n",
        "    (\"Bugs legais\",\"positivo\"),\n",
        "    (\"Resolver\",\"positivo\"),\n",
        "    (\"Eu não\",\"negativo\")\n",
        "]\n",
        "\n",
        "# Passo 2 Pre-Processar o Texto\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "  return re.findall(r'\\b\\w+\\b',text.lower())\n",
        "\n",
        "processed_corpus = [(preprocess_text(text), label) for text, label in corpus]\n",
        "print(processed_corpus)\n",
        "\n"
      ],
      "metadata": {
        "id": "FMwKVN43XM0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2ca79f-50a5-475b-c0d2-5a598b4afd36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['eu', 'amo', 'pln'], 'positivo'), (['eu', 'odeio', 'bugs'], 'negativo'), (['amo', 'resolver', 'problemas'], 'positivo'), (['odeio', 'erros'], 'negativo'), (['bugs', 'legais'], 'positivo'), (['resolver'], 'positivo'), (['eu', 'não'], 'negativo')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 3 - Calculando probabilidades\n",
        "\n",
        "1.\tclass_counts = Counter()\n",
        "\n",
        "  Cria um contador para armazenar a frequência de cada classe no corpus processado.\n",
        "\n",
        "2.\tword_counts = defaultdict(Counter)\n",
        "\n",
        "  Cria um dicionário onde cada chave é uma classe, e o valor associado é outro Counter que conta a frequência de cada palavra nessa classe.\n",
        "\n",
        "3.\ttotal_words = defaultdict(int)\n",
        "\n",
        "\tCria um dicionário onde cada chave é uma classe, e o valor é o total de palavras associadas a essa classe.\n",
        "\n",
        "4.\tfor words, label in processed_corpus:\n",
        "\n",
        "\tInteração pelo corpus processado, que contém listas de palavras (representando os textos) e seus respectivos rótulos (classes).\n",
        "\n",
        "5.\tclass_counts[label] += 1\n",
        "\n",
        "\tIncrementa a contagem da classe label em class_counts.\n",
        "\n",
        "6.\tfor word in words:\n",
        "\n",
        "  Interação pelas palavras de um texto específico.\n",
        "\n",
        "7.\tword_counts[label][word] += 1\n",
        "\n",
        "  Incrementa a contagem da palavra word para a classe label.\n",
        "\n",
        "8.\ttotal_words[label]\n",
        "\n",
        "  Acumula a quantidade total de palavras por classe.\n",
        "\n",
        "9.\ttotal_examples = sum(class_counts.values())\n",
        "\n",
        "  Calcula o número total de exemplos (textos) no corpus.\n",
        "\n",
        "10.\tprior_probabilities = {cls: count / total_examples for cls, count in class_counts.items()}\n",
        "\n",
        "  Calcula as probabilidades a prioridade para cada classe, que é a frequência relativa de cada classe no corpus.\n",
        "\n",
        "11.\tdef conditional_probability(word, label, alpha=1):\n",
        "\n",
        "  Define uma função para calcular a probabilidade condicional de uma palavra dada uma classe, usando suavização de Laplace (alpha=1 por padrão).\n",
        "\n",
        "12. return (word_counts[label][word] + alpha) / (total_words[label] + alpha * len(word_counts[label]))\n",
        "\n",
        "  Esta linha retorna e implementa a probabilidade condicional de uma palavra dada uma classe, com suavização de Laplace.\n"
      ],
      "metadata": {
        "id": "f6hN4ohkD-Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 3 - Calculando probabilidades\n",
        "  # este passo precisa estar explicado\n",
        "class_counts = Counter() # Contagem das classes\n",
        "word_counts = defaultdict(Counter)\n",
        "total_words = defaultdict(int)\n",
        "\n",
        "for words, label in processed_corpus:\n",
        "  class_counts[label] += 1\n",
        "  for word in words:\n",
        "    word_counts[label][word] += 1\n",
        "    total_words[label]\n",
        "\n",
        "total_examples = sum(class_counts.values())\n",
        "prior_probabilities = {cls: count / total_examples for cls, count in class_counts.items()}\n",
        "def conditional_probability(word, label, alpha=1):\n",
        "  return (word_counts[label][word] + alpha) / (total_words[label] + alpha * len(word_counts[label]))\n"
      ],
      "metadata": {
        "id": "IHJ4bY3vDqHA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 4 - Classificar um novo Texto\n",
        "1.\tdef predict(text):\n",
        "\n",
        "\tDefine uma função que recebe um texto e retorna a classe prevista e as probabilidades associadas.\n",
        "\n",
        "2.\twords = preprocess_text(text)\n",
        "\n",
        "\tPré-processa o texto de entrada para transformá-lo em uma lista de palavras (similar ao que foi feito no corpus processado).\n",
        "\n",
        "3.\tprobabilities = {}\n",
        "\n",
        "\tInicializa um dicionário vazio para armazenar as probabilidades calculadas para cada classe.\n",
        "\n",
        "4.\tfor label in class_counts.keys():\n",
        "\n",
        "\tInteração por todas as classes presentes no corpus.\n",
        "\n",
        "5.\tprobabilities[label] = prior_probabilities[label]\n",
        "\n",
        "\tInicializa a probabilidade da classe com sua probabilidade e a prioridade.\n",
        "\n",
        "6.\tfor word in words:\n",
        "\n",
        "\tInteração pelas palavras do texto pré-processado.\n",
        "\n",
        "7.\tprobabilities[label] *= conditional_probability(word, label)\n",
        "\n",
        "\tMultiplica a probabilidade acumulada da classe pela probabilidade condicional de cada palavra no texto dado a classe.\n",
        "\n",
        "8.\treturn max(probabilities, key=probabilities.get), probabilities\n",
        "\n",
        "\tRetorna a classe com maior probabilidade (usando max) e o dicionário completo de probabilidades.\n"
      ],
      "metadata": {
        "id": "8h2-lBvwJt94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 4 - Classificar um novo Texto\n",
        "  # este passo precisa estar explicado\n",
        "def predict(text):\n",
        "  words = preprocess_text(text)\n",
        "  probabilities = {}\n",
        "\n",
        "  for label in class_counts.keys():\n",
        "    probabilities[label] = prior_probabilities[label]\n",
        "    for word in words:\n",
        "      probabilities[label] *= conditional_probability(word, label)\n",
        "  return max(probabilities, key=probabilities.get), probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "I4sz8bOxDv-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 5 realizar o teste com novo texto\n",
        "* Comando:\n",
        "  * print(f'Texto: \"{novo_texto}\"') - imprime o valor do novo texto informado na linha 2 da celula abaixo"
      ],
      "metadata": {
        "id": "Rf4UR7AKkW9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 5 - Teste com um novo texto\n",
        "novo_texto = \"Eu amo resolver bugs\"\n",
        "classe, probs = predict(novo_texto)\n",
        "\n",
        "print(f'Texto: \"{novo_texto}\"')\n",
        "print(f'Classe prevista: `{classe}')\n",
        "print(f'Probabilidades:')\n",
        "for label, prob in probs.items():\n",
        "  print(f\" {label}: {prob}\")"
      ],
      "metadata": {
        "id": "0H3UrzBlkQKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 2 - Classificação do SVM (Support Vector Machines)"
      ],
      "metadata": {
        "id": "qXf7dUXtpnHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQ-8sSKPsKmM",
        "outputId": "680abae1-23b1-46d1-e8d3-bdf0505f5627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1 Importação das bibliotecas a serem utilizadas\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Passo 2 Dados Exemplo\n",
        "corpus = [\n",
        "    \"Eu amo PLN\", \"Eu odeio bugs\", \"Eu amo resolver problemas\",\n",
        "    \"Odeio erros\",\"Amo programação\",\"Não gosto de falhas\"\n",
        "]\n",
        "classes = [\"positivo\",\"negativo\",\"positivo\",\"negativo\",\"positivo\",\"negativo\"]\n",
        "\n",
        "# Passo 3 Pre processamento e vetorização\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "y = classes\n",
        "\n",
        "# Passo 4 Dividir os dados e Treinar o modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Passo 5 Avaliar o modelo\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#              precision    recall  f1-score   support\n",
        "\n",
        "#     negativo       1.00      0.50      0.67         2\n",
        "#     positivo       0.00      0.00      0.00         0\n",
        "\n",
        "#     accuracy                           0.50         2\n",
        "#    macro avg       0.50      0.25      0.33         2\n",
        "# weighted avg       1.00      0.50      0.67         2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxvjx7Zqpu_g",
        "outputId": "af39f4ee-ade2-4da5-d92b-c1762bd6e524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       1.00      1.00      1.00         1\n",
            "    positivo       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 3 - Comparando os Classificadores com Scikit-learn"
      ],
      "metadata": {
        "id": "VCTl_bF1uBag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Importar Bibliotecas\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "  # Baixar o dataset de exemplo\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# 2. Preparação dos dados\n",
        "\n",
        "  # Coleta de textos e classes\n",
        "documents = [(\" \".join(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "  # Separar textos e rótulos\n",
        "texts, labels = zip(*documents)\n",
        "\n",
        "  # Transformar rótulos (positivo/negativo) em 0 e 1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "  # Dividir dados em treino e teste\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Representação do texto com TF-IDF\n",
        "  # Criar o vetorizador TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limitar a 5.000 palavras mais comuns\n",
        "\n",
        "  # Ajustar e transformar os textos\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "X_test = vectorizer.transform(texts_test)\n",
        "\n",
        "# 4. Treinar os modelos\n",
        "\n",
        "  # Treinamento do Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, labels_train)\n",
        "\n",
        "  # Predição\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "  # Treinamento do SVM\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, labels_train)\n",
        "\n",
        "  # Predição\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Avaliação\n",
        "\n",
        "  # Avaliação do Naive Bayes\n",
        "print(\"Naive Bayes Performance:\")\n",
        "print(classification_report(labels_test, nb_predictions, target_names=label_encoder.classes_))\n",
        "\n",
        "  # Avaliação do SVM\n",
        "print(\"SVM Performance:\")\n",
        "print(classification_report(labels_test, svm_predictions, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AnAv44ouL3M",
        "outputId": "53c3a840-61ad-4eba-fd30-7be4beaf7eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.79      0.84      0.81       302\n",
            "         pos       0.82      0.77      0.80       298\n",
            "\n",
            "    accuracy                           0.80       600\n",
            "   macro avg       0.80      0.80      0.80       600\n",
            "weighted avg       0.80      0.80      0.80       600\n",
            "\n",
            "SVM Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.82      0.80      0.81       302\n",
            "         pos       0.81      0.82      0.81       298\n",
            "\n",
            "    accuracy                           0.81       600\n",
            "   macro avg       0.81      0.81      0.81       600\n",
            "weighted avg       0.81      0.81      0.81       600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}